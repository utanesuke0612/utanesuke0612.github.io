---
layout: post
title: 深度学习入门-05-误差反向传播法
date: 2018-07-28 00:00:05
categories: DeepLearning
tags: DeepLearning
---
* content
{:toc}

上一章介绍了神经网络的学习，并通过数值微分计算了神经网络的权重参数的梯度(损失函数关于权重参数的梯度)，数值微分虽然简单容易实现，但是比较费时间。本站介绍能高效计算权重参数的梯度的方法-误差反向传播法。

# 1. 计算图

计算题通过节点和箭头表示计算过程，节点用O表示,O中是计算的内容，计算中间结果在箭头上方。

![image](https://user-images.githubusercontent.com/18595935/43989774-50de30a6-9d8b-11e8-97dc-6891df5aae6f.png)

使用计算图最大的原因是 **可以通过反向传播高效计算导数**。

在介绍计算图的反向传播时，思考一个如下的例子，计算购买2个苹果时加上消费税最终需要支付的金额。

![image](https://user-images.githubusercontent.com/18595935/43989882-8e822ece-9d8d-11e8-8c37-23cf72a4e03c.png)

假设我们这里想知道苹果价格上涨会多大程度上影响支付金额，即支付金额关于苹果的价格导数，上图可知是2.2，其他因素，比如个数和消费税对支付金额的影响，也可以通过类似的方式求得。

# 2. 链式法则

局部导数的反向传递，是基于**链式法则(chain rule)**.

![image](https://user-images.githubusercontent.com/18595935/43989945-cdbf0930-9d8e-11e8-8c98-8a440dc2c4f0.png)

如上图所示，反向传播的计算顺序，将信号E乘以节点的局部导数(y关于x的导数)，假设y=f(x)=x**2，则局部导数为2x.

## 2.1 链式法则

链式法则是关于符合函数的导数的性质: *如果某个函数由复合函数表示，则该复合函数的导数可以用构成复合函数的各个函数的导数乘积表示。*

![image](https://user-images.githubusercontent.com/18595935/43990269-c2592214-9d94-11e8-89f2-dece6035f46d.png)


# 3. 反向传播

# 4. 简单层的思想

# 5. 激活函数层的思想

# 6. Affine-Softmax层的实现



