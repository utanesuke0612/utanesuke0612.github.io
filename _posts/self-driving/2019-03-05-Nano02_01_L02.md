---
layout: post
title: Nano02(自動運転)-U01-L02-定位入门
date: 2019-04-06 01:02:00
categories: self-driving(自動運転)
tags: self-driving
---
* content
{:toc}

# 0. 小结

1. 无人驾驶的定位方式：通过自身传感器，感知周边物体的距离与方向，然后与高精度地图匹配，精度结果要10cm误差以下。
2. 感知后的概率，即通过传感器结果计算出可能位置的概率，最后将这些概率进行正规化(求和为1)，这个函数是sense(p,Z)是定位中的关键函数，被称作度量更新。
3. move函数实现往右移动U个单位，并计算移动后的概率，move(p, U)。
4. 移动的时候，也存在概率不同的问题， 比如移动1格概率为0.8，向后向前一格的概率为0.1，通过move函数能根据移动的概率，更新最后的概率分布。
5. 如果只是不断的循环移动，那最终是个平均分布，其熵值上升，通过感知后，熵值下降，熵表示一个系统的不确定性。
6. 定位就是上述：SENSE和MOVE的迭代，每一次机器人移动后，它会失去它在哪里的信息，因为机器人的移动是不准确的，而每一次感知，它会获得信息。
7. 贝叶斯规则 `P(A|B) = P(B|A)*P(A) / P(B)`
8. 全概率公式：全概率公式将对一复杂事件A的概率求解问题，转化为了在B下发生的A事件的概率，与B发生概率乘积，最后求和的问题。详细见下面[38. Theorem of Total Probability 全概率]的公式。

# 0.1. 先验分布与后验分布

> 隔壁小哥要去15公里外的一个公园，他可以选择步行走路，骑自行车或者开辆车，然后通过其中一种方式花了一段时间到达公园。
> 首先在这个事里边，大家不要关注隔壁小哥去干嘛，也许去送外卖吧：) 。言归正传，这件事中采用哪种交通方式是因，花了多长时间是果。俗话说瓜熟蒂落，皆是因果；因果循环，报应不爽。要理解即将提到的概念，何为因何为果先要搞清楚。

1. **后验概率**：这种预先已知结果（路上花的时间），然后根据结果估计（猜）原因（交通方式）的概率分布即 后验概率。
2. **先验概率**：根据历史规律确定原因 （交通方式）的概率分布即 先验概率。


# 5. Localizing a Self Driving Car

首先无人驾驶的定位要解决的问题是：确定自身车辆在指定地图上的位置，而且要小于10cm的误差的精度。

通过传统的GPS进行定位的话，一般误差在1-3m，如果是城市中误差可能达到10-50m，显然这种方式是不可取的。

这里的做法是，使用车载传感器，比如RADAR或LIDAR，对周围的静态物体进行测量：
1. 测量车与静态物体的距离
2. 测量静态物体的方向

然后将上面的距离和方向信息(自车的坐标系)，转换到与地图相同坐标系，并进行匹配，匹配误差要求小于10cm。

![image](https://user-images.githubusercontent.com/18595935/53855973-40f70e80-4013-11e9-9532-a549314a0cba.png)

# 6. 课程概要

1. 介绍什么是定位，并用python进行实现
2. 深入学习贝叶斯滤波器背后的数学知识，然后用C++实现一个一维模型
3. 学习运动模型
4. 使用C++完成二维粒子滤波器

# 10. Generalized Uniform Distribution 广义平均分布

```python
#  Modify your code to create probability vectors, p, of arbitrary 
#  size, n. Use n=5 to verify that your new solution matches 
#  the previous one.

p=[]
n=5

for i in range(n):
    p.append(1.0/n)

print p
```

```
[0.2, 0.2, 0.2, 0.2, 0.2]
```

# 11. Probability After Sense 感知后的概率

未感知之前，汽车在这5个框中的概率相同，都是0.2；但是感知之后，比如车感知到当前位于红色区域块，那重新设置置信度，红色乘以0.6，绿色乘以0.2，得到新的概率：

![image](https://user-images.githubusercontent.com/18595935/53856887-671ead80-4017-11e9-8b5b-aa29e4c7286c.png)

# 13. Normalize Distribution 正态分布

经过上面的置信度重新计算后，概率和不为1了，需要重新进行正态分布：

![image](https://user-images.githubusercontent.com/18595935/53857032-0ba0ef80-4018-11e9-8657-812a219b85dc.png)

# 16. Sense Function

```python
#Modify the code below so that the function sense, which 
#takes p and Z as inputs, will output the NON-normalized 
#probability distribution, q, after multiplying the entries 
#in p by pHit or pMiss according to the color in the 
#corresponding cell in world.
p=[0.2, 0.2, 0.2, 0.2, 0.2]
world=['green', 'red', 'red', 'green', 'green']
Z = 'red'
pHit = 0.6
pMiss = 0.2

def sense(p, Z):
    #
    #ADD YOUR CODE HERE
    #
    q = []

    for i in range(len(p)):
        if Z == world[i]:
            q.append(pHit*p[i])
        else:
            q.append(pMiss*p[i])
    return q           

print sense(p,Z)
```

```
[0.04000000000000001, 0.12, 0.12, 0.04000000000000001, 0.04000000000000001]
```

# 17. Normalized Sense Function

下面是定位中的关键函数，被称作度量更新(measurement update)，下面这个函数根据Z进行判断，然后添加到q中的实现方式比较巧妙：

```python
#Modify your code so that it normalizes the output for 
#the function sense. This means that the entries in q 
#should sum to one.
p=[0.2, 0.2, 0.2, 0.2, 0.2]
world=['green', 'red', 'red', 'green', 'green']
Z = 'red'
pHit = 0.6
pMiss = 0.2

def sense(p, Z):
    q=[]
    for i in range(len(p)):
        hit = (Z == world[i])
        q.append(p[i] * (hit * pHit + (1-hit) * pMiss))
    
    q = [x/sum(q) for x in q]
        
    return q
print sense(p,Z)

```

```
[0.1111111111111111, 0.3333333333333332, 0.3333333333333332, 0.1111111111111111, 0.1111111111111111]
```

# 19. Multiple Measurements

感知到了可能是红色和绿色，那么需要进行两次更新，详细代码如下，可以看到最终结果概率均为0.2，说明这5个概率又相同了：

```python
#Modify the code so that it updates the probability twice
#and gives the posterior distribution after both 
#measurements are incorporated. Make sure that your code 
#allows for any sequence of measurement of any length.

p=[0.2, 0.2, 0.2, 0.2, 0.2]
world=['green', 'red', 'red', 'green', 'green']
measurements = ['red', 'green']
pHit = 0.6
pMiss = 0.2

def sense(p, Z):
    q=[]
    for i in range(len(p)):
        hit = (Z == world[i])
        q.append(p[i] * (hit * pHit + (1-hit) * pMiss))
    s = sum(q)
    for i in range(len(q)):
        q[i] = q[i] / s
    return q
#
#ADD YOUR CODE HERE

for z in measurements:
    p = sense(p,z)

#
print p
```

```
[0.20000000000000004, 0.19999999999999996, 0.19999999999999996, 0.20000000000000004, 0.20000000000000004]
```

# 20. Move Function

下面实现了移动函数，将概率往右移动U个单位：

```python
#Program a function that returns a new distribution 
#q, shifted to the right by U units. If U=0, q should 
#be the same as p.

p=[0, 1, 0, 0, 0]
world=['green', 'red', 'red', 'green', 'green']
measurements = ['red', 'green']
pHit = 0.6
pMiss = 0.2

def sense(p, Z):
    q=[]
    for i in range(len(p)):
        hit = (Z == world[i])
        q.append(p[i] * (hit * pHit + (1-hit) * pMiss))
    s = sum(q)
    for i in range(len(q)):
        q[i] = q[i] / s
    return q

def move(p, U):
    #
    #ADD CODE HERE
    q = []
    for i in range(len(p)):
        q.append(p[i-U] % len(p)) 
    #
    return q

print move(p, 1)

```

```
[0, 0, 1, 0, 0]
```

还可以如下实现方式：

```python
U = U % len(p)
q = p[-U:] + p[:-U]
```

# 23. Inexact Motion 1

但移动到某一格的概率是不一样的，如下面1移动1格和3格概率为0.1，移动2格概率为0.2：

![image](https://user-images.githubusercontent.com/18595935/53860119-4e1bf980-4023-11e9-9f5d-78b5262af938.png)

# 24. Inexact Move Function

通过代码实现，准确移动到指定位置的概率为0.8，位于前方或后方的概率是0.1.

```python
#Modify the move function to accommodate the added 
#probabilities of overshooting or undershooting 
#the intended destination.

p=[0, 1, 0, 0, 0]
world=['green', 'red', 'red', 'green', 'green']
measurements = ['red', 'green']
pHit = 0.6
pMiss = 0.2
pExact = 0.8
pOvershoot = 0.1
pUndershoot = 0.1

def sense(p, Z):
    q=[]
    for i in range(len(p)):
        hit = (Z == world[i])
        q.append(p[i] * (hit * pHit + (1-hit) * pMiss))
    s = sum(q)
    for i in range(len(q)):
        q[i] = q[i] / s
    return q

def move(p, U):
    q = []
    for i in range(len(p)):
        s = pExact * (p[(i-U)%len(p)])
        s += pOvershoot * (p[(i-U+1)%len(p)])
        s += pUndershoot * (p[(i-U-1)%len(p)])
        q.append(s)
    return q
    
print move(p, 1)
```

# 29. Move 1000

按照上面的方式如果循环移动无限次之后，最后的结果会是5个格子的概率相同，下面的代码是1000次移动后的结果：

```python
#write code that moves 1000 times and then prints the 
#resulting probability distribution.

p=[0, 1, 0, 0, 0]
world=['green', 'red', 'red', 'green', 'green']
measurements = ['red', 'green']
pHit = 0.6
pMiss = 0.2
pExact = 0.8
pOvershoot = 0.1
pUndershoot = 0.1

def sense(p, Z):
    q=[]
    for i in range(len(p)):
        hit = (Z == world[i])
        q.append(p[i] * (hit * pHit + (1-hit) * pMiss))
    s = sum(q)
    for i in range(len(q)):
        q[i] = q[i] / s
    return q

def move(p, U):
    q = []
    for i in range(len(p)):
        s = pExact * p[(i-U) % len(p)]
        s = s + pOvershoot * p[(i-U-1) % len(p)]
        s = s + pUndershoot * p[(i-U+1) % len(p)]
        q.append(s)
    return q
#

for i in range(1000):
    p = move(p,1)
#
print p

```

```python
[0.20000000000000365, 0.20000000000000373, 0.20000000000000365, 0.2000000000000035, 0.2000000000000035]
```

# 30. Sense and Move

定位就是上述：**SENSE**和**MOVE**的迭代，每一次机器人移动后，它会失去它在哪里的信息，因为机器人的移动是不准确的，而每一次感知，它会获得信息。

这个循环就是不断地失去信息，再获取信息。

关于Entropy熵的概念：

![image](https://user-images.githubusercontent.com/18595935/53862406-a7d3f200-402a-11e9-8b1a-dec8d06b9012.png)

经过感知后，熵值下降；移动后，熵值上升；熵一般用于衡量系统的不确定性，比如现在这个例子中概率为 [0.2, 0.2, 0.2, 0.2, 0.2]，那么其熵值为：

```python
-5 * (0.2) * log(0.2) = 0.699
```

如果经过一次感知即测量后，概率更新为[0.05, 0.05, 0.05, 0.8, 0.05]，熵求得为0.338.


下面的代码中，先感知再移动，分布进行了两次：


```python
#Given the list motions=[1,1] which means the robot 
#moves right and then right again, compute the posterior 
#distribution if the robot first senses red, then moves 
#right one, then senses green, then moves right again, 
#starting with a uniform prior distribution.

p=[0.2, 0.2, 0.2, 0.2, 0.2]
world=['green', 'red', 'red', 'green', 'green']
measurements = ['red', 'green']
motions = [1,1]
pHit = 0.6
pMiss = 0.2
pExact = 0.8
pOvershoot = 0.1
pUndershoot = 0.1

def sense(p, Z):
    q=[]
    for i in range(len(p)):
        hit = (Z == world[i])
        q.append(p[i] * (hit * pHit + (1-hit) * pMiss))
    s = sum(q)
    for i in range(len(q)):
        q[i] = q[i] / s
    return q

def move(p, U):
    q = []
    for i in range(len(p)):
        s = pExact * p[(i-U) % len(p)]
        s = s + pOvershoot * p[(i-U-1) % len(p)]
        s = s + pUndershoot * p[(i-U+1) % len(p)]
        q.append(s)
    return q
#
# ADD CODE HERE
for i in range(len(measurements)):
    p = sense(p,measurements[i])
    p = move(p,motions[i])
#
print p         

```

```python
[0.21157894736842103, 0.1515789473684211, 0.08105263157894739, 0.16842105263157897, 0.3873684210526316]
```

# 32. Localization Summary

1. Belief = 概率
2. Sense = 感知到的概率与现有概率的乘积，然后正规化处理
3. Move = 类似于一个卷积，不断累加概率

# 36. Bayes' Rule 贝叶斯规则

下面就是贝叶斯规则：

```
P(A|B) = P(B|A)*P(A) / P(B)
```

1. P(X/Z) 表示在观察到度量之后，车位置的信度。
2. P(Z/X) * P(X) 表示每个可能的位置上乘上看到红色或绿色的概率，P(X)是该格子的先验概率，P(Z/X)是该格子的度量概率。
3. P(Z) 是上面分子，在i个上的求和。

![image](https://user-images.githubusercontent.com/18595935/53871124-ed9ab580-403e-11e9-9a15-dc798942f9f9.png)


# 37. Cancer Test

参考上面的公式，计算P(C/POS)的概率，即阳性情况下是癌症的几率：

1. 假设 癌症C / 非癌症not C 是 机器人的位置
2. 检测阳性作为是否被观察的颜色是正确的颜色

![image](https://user-images.githubusercontent.com/18595935/53871606-e88a3600-403f-11e9-8c25-90eba1392f97.png)

# 38. Theorem of Total Probability 全概率

此处Pr(A / B)是B发生后，A发生的条件概率，所以全概率公式又可写作：

![image](https://user-images.githubusercontent.com/18595935/53873528-e4f8ae00-4043-11e9-97ad-be027b66cf13.png)

全概率公式将对一复杂事件A的概率求解问题转化为了在不同情况或不同原因 Bn下发生的简单事件的概率的求和问题。

# 39. Coin Flip Quiz

1. 硬币出现正面反面概率一样，都是0.5
2. 如果是反面T，则接受；如果是正面H，则再扔一次并接受

![image](https://user-images.githubusercontent.com/18595935/53873837-8bdd4a00-4044-11e9-8e39-dc80c8b37aa7.png)

可以使用上面的全概率公式，存在两种case：
1. 第一次为H，并且第二次仍是H的情况，概率为0.5×0.5
2. 第一次为T，并且第二次为H的情况，因为第一次为T就直接接受了，所以概率为0
3. 最终结果为上面的和 0.25


# 40. Two Coin Quiz

1. 假设有两种硬币，一种是正常的，出现H和T的概率都是0.5，另一种是不公平的，出现H的概率是0.1
2. 随机选取上面的两种硬币，几率都是0.5
3. 如果选取的硬币抛出后是H，那它是正常硬币概率是多少？

通过贝叶斯规则和全概率公式：
1. `P(f/H) = (P(H/f)*P(f) / P(H))`
2. `P(H) = P(H|f) + P(H|u) = P(f)*P(H) + P(u)*P(H) = 0.5*0.5 + 0.5*0.1 = 0.3`
3. `P(H/f)*P(f) = 0.5 * 0.5 = 0.25`

![image](https://user-images.githubusercontent.com/18595935/53874936-d65fc600-4046-11e9-8dad-af798ad67eb0.png)

