---
layout: post
title: Nano01(自動運転)-U03-Lesson16-Project-Traffic Sign Classifier
date: 2019-01-01 02:06:04
categories: self-driving(自動運転)
tags: self-driving
---
* content
{:toc}

这个project中需要构建一个深度神经网络，对交通标志进行分类，比如是停止或是前行标志。

下面内容是Lesson15中的第9和10，讲解了如何通过LeNet来完成这个project。


# 1. LeNet for Traffic Signs

## 1.1 LeNet for Traffic Signs

可以使用上一章的神经网络去训练交通信号灯识别：

1. Reset the kernel and clear the output to ensure you have a fresh start.
2. Clear the cells that load the MNIST data and replace it with code to load the traffic sign data.(the code is in the notebook for the traffic sign classifier project)
3. The traffic sign data does not come with a validation set, you can use the train test split function in the SKLearn library though to slice off a validation set from the training set.

**You can proceed to pre-process the data and set up TensorFlow just as we dit for MNIST.**, but you'll have to make a few changes to the LeNet architecture though.

1. The traffic sign images are in color not gray scale, so the input depth should be three to match the three RGB color channels.
2. At the ouput layer the traffic sign classifier has 43 classes, whereas MNIST only had 10 so you'll have to change that.
3. When you set up the TensorFlow placeholders, you'll have to update the input and output shapes again to match the shapes of the new dataset.

通过上述的改造，就可以大致使用该神经网络去训练信号灯数据了，下面是进一步的改进方法：
1. experiment with different network architectures, or just change the dimensions of the LeNet layers
2. add regularization features like drop out or L2 regularization to make sure the network doesn't overfit the training data
3. tune the hyperparameters
4. improve the data pre-processing with steps like normalization and setting a zero mean
5. augment the training data by rotating or shifting images or by changing colors

## 1.2 Visualizing Layers

神经网络对于我们训练来说类似一个黑盒，通过一些技巧将各个层的信息可视化，可以看出神经网络究竟对什么样的信息感兴趣。

下面的这个函数，能将TensorFlow中指定的权重层可视化：
- 输入数据1：图片数据，可以是训练中的数据，也可以是提供的input
- 输入数据2：the Tensorflow variable name that represents the layer's state during the training process

如果是想确认LeNet中第二层卷积层的特征图，输入数据则是**conv2 **和**tf_activation **。

示例代码如下：

```python
# image_input: the test image being fed into the network to produce the feature maps
# tf_activation: should be a tf variable name used during your training procedure that represents the calculated state of a specific weight layer
# Note: that to get access to tf_activation, the session should be interactive which can be achieved with the following commands.
# sess = tf.InteractiveSession()
# sess.as_default()

# activation_min/max: can be used to view the activation contrast in more detail, by default matplot sets min and max to the actual min and    max values of the output
# plt_num: used to plot out multiple different weight feature map sets on the same block, just extend the plt number for each new feature map entry

def outputFeatureMap(image_input, tf_activation, activation_min=-1, activation_max=-1 ,plt_num=1):
    # Here make sure to preprocess your image_input in a way your network expects
    # with size, normalization, ect if needed
    # image_input =
    # Note: x should be the same name as your network's tensorflow data placeholder variable
    # If you get an error tf_activation is not defined it maybe having trouble accessing the variable from inside a function
    activation = tf_activation.eval(session=sess,feed_dict={x : image_input})
    featuremaps = activation.shape[3]
    plt.figure(plt_num, figsize=(15,15))
    for featuremap in range(featuremaps):
        plt.subplot(6,8, featuremap+1) # sets the number of feature maps to show on each row and column
        plt.title('FeatureMap ' + str(featuremap)) # displays the feature map number
        if activation_min != -1 & activation_max != -1:
            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", vmin =activation_min, vmax=activation_max, cmap="gray")
        elif activation_max != -1:
            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", vmax=activation_max, cmap="gray")
        elif activation_min !=-1:
            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", vmin=activation_min, cmap="gray")
        else:
            plt.imshow(activation[0,:,:, featuremap], interpolation="nearest", cmap="gray")
```

# Overview

In this project, I have implemented a classifier using deep neural networks, convolutional neural networks and transfer learning to classify traffic signs. I have trained a model so that it can decode traffic signs from natural images by using the German Traffic Sign Dataset. After the model is trained, I have tested my model program on new images of traffic signs collected from the web.

The steps of this project are the following:

- Load the data set (see below for links to the project data set)
- Explore, summarize and visualize the data set
- Design, train and test a model architecture
- Use the model to make predictions on new images
- Analyze the softmax probabilities of the new images
- Summarize the results with a written report

# Data Set Summary & Exploration

This is a pickled dataset in which we've already resized the images to 32x32.

## Provide a basic summary of the data set

I used the numpy library to calculate summary statistics of the traffic signs data set:

- The size of training set is 34799
- The size of validation set is 4410
- The size of test set is 12630
- The shape of a traffic sign image is (32, 32, 3)
- The number of unique classes/labels in the data set is 43

## Include an exploratory visualization of the dataset 

Here is an exploratory visualization of the data set. It pulls in a random set of 5 images.

![image](https://user-images.githubusercontent.com/18595935/52328076-b3cf9280-2a31-11e9-9766-3e537ee4e453.png)

I also detail the dataset structure by plotting the occurrence of each image class to get an idea of how the data is distributed. 

![image](https://user-images.githubusercontent.com/18595935/52328149-ebd6d580-2a31-11e9-8ba8-e694c6219a25.png)

# Design and Test a Model Architecture

My final model consisted of the following layers:

|Layer|Description|
|:--|--:|
|Input|32x32x3 color image|
|Convolution 5x5	|2x2 stride, valid padding, outputs 28x28x6|
| RELU	||
| Max pooling	| 2x2 stride, outputs 14x14x6|
| Convolution 5x5	|2x2 stride, valid padding, outputs 10x10x16|
| RELU	||
| Max pooling	|2x2 stride, outputs 5x5x6|
| Fully connected |input 400, output 120|
| RELU	||
| Dropout		| 50% keep |
| Fully connected	| input 120, output 84 |
| RELU	||
| Dropout | 50% keep |
|Fully connected| input 84, output 43 |

To train the model, I used an LeNet for the most part that was given, but I added a dropout function for deleteing the fully connected layer's output. I used the AdamOptimizer with a learning rate of 0.001. The epochs used was 30 while the batch size was 64. Other important parameters I learned were important was the number and distribution of additional data generated. 

**My final model results were:**
- Train Accuracy = 0.999
- Valid Accuracy = 0.968
- Test Accuracy = 0.946

# Test a Model on New Images

Here are five German traffic signs that I found on the web:

![image](https://user-images.githubusercontent.com/18595935/52329267-9b617700-2a35-11e9-97c8-2a28e93e1679.png)

The model was able to correctly guess 8 of the 8 traffic signs, which gives an accuracy of 100%.

![image](https://user-images.githubusercontent.com/18595935/52329449-22165400-2a36-11e9-9057-ca5db33021f8.png)

