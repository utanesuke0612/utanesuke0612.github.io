---
layout: post
title: Nano01(自動運転)-U03-Lesson14-Convolutional Neural Networks 卷积神经网络
date: 2019-01-01 02:04:04
categories: self-driving(自動運転)
tags: self-driving
---
* content
{:toc}

> 本章参考[深度学习入门-07-卷积神经网络](http://road2ai.info/2018/07/28/Deeplearning_07/)


# 4. 

# 6. Intuition

一个CNN会有很多层，每个层上会有不同的特征，这些层上的特征都是CNN自身学习得到的，不需要人为去指定各个层的特征。

# 7. Filters 滤波器

CNN神经网络的第一步，就是将图片分割成更小的块，这个分割的过程通过滤波器完成。

这个分割过程，在这里有详细讲解[卷积运算](http://road2ai.info/2018/07/28/Deeplearning_07/#22-%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97)

将滤波器应用在输入数据即图片上，按照步长(stride)进行移动，不断进行矩阵乘积运算，得到新的输出，一般来说增加步长会导致model的size变小，减少了一个layer上的神经元数量，最终导致精度降低。

**Filter Depth(滤波器深度)：**

一般来说滤波器不止一个，不同的滤波器应用到不同的通道上(R/G/B)，具体可以参考[3维数据的卷积运算](http://road2ai.info/2018/07/28/Deeplearning_07/#25-3%E7%BB%B4%E6%95%B0%E6%8D%AE%E7%9A%84%E5%8D%B7%E7%A7%AF%E8%BF%90%E7%AE%97)。

# 8. Feature Map Sizes

![image](https://user-images.githubusercontent.com/18595935/52128007-2ed22b00-2677-11e9-8b0b-9f52c433402e.png)

- input depth 表示输入数据有3个通道
- output depth 表示输出数据有8个通道，说明滤波器是个四维的数据，size为(8,3,3,3)，表示意义为(FN滤波器个数，C通道，FH高，FW宽)
- Padding是**same**，表示输入数据会有填充，以保证输出数据的高宽，与输入数据一样，所以，padding为same，步幅stride是1时，三个值分别是(28,28,8)
- Padding是**valid**，表示没有填充，滤波器移动时不超过边界，根据下面的公式计算，如果步长stride为1，则结果为(26,26,8);步长为2，则结果为(13,13,8)。

![image](https://user-images.githubusercontent.com/18595935/52128885-7659b680-2679-11e9-9296-a9127bd55f64.png)

另外，多个滤波器的卷积元素，参考下图：

![image](https://user-images.githubusercontent.com/18595935/52104251-e857de80-262c-11e9-8f04-21633df17707.png)

**Same padding / valid padding:**

Same padding表示会对输入数据进行填充，使得输出数据与输入数据有相同的高和宽，而valid padding是不会填充，移动时不会超过边界。

![image](https://user-images.githubusercontent.com/18595935/52128763-2549c280-2679-11e9-8bdf-81464ea3b973.png)

# 9. Convolutions continued

![image](https://user-images.githubusercontent.com/18595935/52129338-9d64b800-267a-11e9-86df-0113c26ca548.png)

卷积层计算完毕后，需要将其变换成全连接层(Full connected layer)，然后训练分类器。

如何变换成全连接层，可以参考[基于im2col的展开](http://road2ai.info/2018/07/28/Deeplearning_07/#42-%E5%9F%BA%E4%BA%8Eim2col%E7%9A%84%E5%B1%95%E5%BC%80)

> A "Fully Connected" layer is a standard, non convolutional layer, where all inputs are connected to all output neurons. This is also referred to as a "dense" layer, and is what we used in the previous two lessons.

# 10. Parameters

一般来说，我们在识别的时候，不需要理会识别对象所在的问题，不管是在左上或是右下，在我们眼中都是该对象。
我们希望CNN也有相同的处理能力(平移不变性translation invariance)。

如之前描述的，图片上给定patch的分类，由该patch对应的权重和偏置所决定。
所以，如果希望左上角的猫，和右下角的猫，都能被CNN识别为猫，那他们需要相同的权重和偏置。

> 实际上上面解释的，可以参考[卷积层](http://road2ai.info/2018/07/28/Deeplearning_07/#2-卷积层)，详细的讲解了步长，填充，以及卷积运算的步骤。
> 

**Dimensionality 维度：**

```
1. input layer has a width of W and a height of H
2. our convolutional layer has a filter size F
3. the number of filters : K
4. stride ： S
5. padding : P
```

1. the following formula gives us the width of the next layer: **W_out =[ (W−F+2P)/S] + 1**
2. The output height would be **H_out = [(H-F+2P)/S] + 1**
3. the output depth would be equal to the number of filters **D_out = K**
4. The output volume would be `W_out * H_out * D_out`

# 11. Quiz: Convolution Output Shape

```
- We have an input of shape 32x32x3 (HxWxD)
- 20 filters of shape 8x8x3 (HxWxD)
- A stride of 2 for both the height and width (S)
- With padding of size 1 (P)
```

根据下面的公式：

```
new_height = (input_height - filter_height + 2 * P)/S + 1
new_width = (input_width - filter_width + 2 * P)/S + 1
```

计算出output的shape为：`14x14x20.`

上面的实现过程用TensorFlow代码如下：

```python
input = tf.placeholder(tf.float32, (None, 32, 32, 3))
filter_weights = tf.Variable(tf.truncated_normal((8, 8, 3, 20))) # (height, width, input_depth, output_depth)
filter_bias = tf.Variable(tf.zeros(20))
strides = [1, 2, 2, 1] # (batch, height, width, depth)
padding = 'SAME'
conv = tf.nn.conv2d(input, filter_weights, strides, padding) + filter_bias
```

通过上面的代码计算出来的conv的形状为`[1,16,16,20]`，并不与我们预期的`[1,14,14,20]`相同，这是因为TensorFlow中的`SAME`和`VALID`，与之前讲解的不同。

如果将SAME修改为VALID，最终conv的shape为`[1,13,13,20]`。

TensorFlow中关于SAME和VALID下output的shape计算方式如下：

**SAME Padding：**

```
out_height = ceil(float(in_height) / float(strides[1]))
out_width = ceil(float(in_width) / float(strides[2]))
```

**VALID Padding：**

```
out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))
out_width = ceil(float(in_width - filter_width + 1) / float(strides[2]))
```

# 13. Quiz: Number of Parameters

```
- We have an input of shape 32x32x3 (HxWxD)
- 20 filters of shape 8x8x3 (HxWxD)
- A stride of 2 for both the height and width (S)
- With padding of size 1 (P)
```

output layer: `14x14x20 (HxWxD)`

**Hint：**

Without parameter sharing, each neuron in the output layer must connect to each neuron in the filter. In addition, each neuron in the output layer must also connect to a single bias neuron.


参考下面的图，可以计算得到，如果没有权重共享的话，将会有`(8 * 8 * 3 + 1) * (14 * 14 * 20) = 756560`个参数。
> `8 * 8 * 3`是权重的个数，1是表示bias个数，右边的`14 * 14 * 20`是连接着的输出层的神经元数量。

![image](https://user-images.githubusercontent.com/18595935/43671248-5cbd44e4-97d1-11e8-9647-f430f768549e.png)

# 15. Quiz: Parameter Sharing

同样是上面的条件，如果有权值共享的话，那么参数的个数是多少呢？

```
- We have an input of shape 32x32x3 (HxWxD)
- 20 filters of shape 8x8x3 (HxWxD)
- A stride of 2 for both the height and width (S)
- With padding of size 1 (P)
```

output layer: `14x14x20 (HxWxD)`

**Hint：**

With parameter sharing, each neuron in an output channel shares its weights with every other neuron in that channel. So **the number of parameters** is equal to **the number of neurons in the filter**, **plus a bias neuron**, all multiplied by **the number of channels in the output layer**.

结果是：

```
(8 * 8 * 3 + 1) * 20 = 3840 + 20 = 3860
```

> 3840是权重，20是偏置，在权重共享中，我们在一个通道上使用同一个filter即权重，可以参考下图。

![image](https://user-images.githubusercontent.com/18595935/52103519-5dc1b000-2629-11e9-8721-acd997b38e37.png)

# 17. Visualizing CNNs

> CNN的可视化，对应[CNN的可视化](http://road2ai.info/2018/07/28/Deeplearning_07/#6-cnn的可视化)

下面的例子来自于ImageNet中的识别结果，列举Layer1-Layer5的识别出的特征：

**Layer1：**

![image](https://user-images.githubusercontent.com/18595935/52162063-cb490b80-2711-11e9-884d-9dd973c97355.png)

**Layer2：**

![image](https://user-images.githubusercontent.com/18595935/52162068-d308b000-2711-11e9-96f3-25e95d8dceeb.png)

**Layer3：**

![image](https://user-images.githubusercontent.com/18595935/52162073-d9972780-2711-11e9-94d6-8c673e2a6fc2.png)

**Layer5：**

![image](https://user-images.githubusercontent.com/18595935/52162077-e451bc80-2711-11e9-8e48-e1e7c77ec52b.png)

可以看到，随着层次的加深，其特征也越来越明显。

# 18. TensorFlow Convolution Layer

下面看看TensorFlow中如何实施CNN，可以使用 tf.nn.conv2d() and tf.nn.bias_add()去生成卷积层：

```python
# Output depth
k_output = 64

# Image Properties
image_width = 10
image_height = 10
color_channels = 3

# Convolution filter
filter_size_width = 5
filter_size_height = 5

# Input/Image
input = tf.placeholder(
    tf.float32,
    shape=[None, image_height, image_width, color_channels])

# Weight and bias
weight = tf.Variable(tf.truncated_normal(
    [filter_size_height, filter_size_width, color_channels, k_output]))
bias = tf.Variable(tf.zeros(k_output))

# Apply Convolution
conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')
# Add bias
conv_layer = tf.nn.bias_add(conv_layer, bias)
# Apply activation function
conv_layer = tf.nn.relu(conv_layer)
```

1. `tf.nn.conv2d()`: 利用滤波器(权重)和步长，以及填充方式作为input，生成卷积层
2. `strides=[1, 2, 2, 1]`: strides: A list of ints. 1-D tensor of length 4. The stride of the sliding window for each dimension of input. For the most common case of the same horizontal and vertices strides, strides = [1, stride, stride, 1].
3. `input` : Given an input tensor of shape [batch, in_height, in_width, in_channels]
4. `tf.nn.bias_add()`：adds a 1-d bias to the last dimension in a matrix.

# 19. Explore The Design Space

主要介绍了池化层，详细可以参考[池化层](http://road2ai.info/2018/07/28/Deeplearning_07/#3-池化层)

# 20. TensorFlow Max Pooling

![image](https://user-images.githubusercontent.com/18595935/52162298-131d6200-2715-11e9-8f0c-cbce4a3a7178.png)

如上图，是Max池化，以2*2滤波器和2的步长，对input进行处理，得到右边的结果。

通过池化处理：能减少input的数据量，将最主要的值保留下来。

TensorFlow中使用`tf.nn.max_pool()`函数实现卷积层的池化：

```python
...
conv_layer = tf.nn.conv2d(input, weight, strides=[1, 2, 2, 1], padding='SAME')
conv_layer = tf.nn.bias_add(conv_layer, bias)
conv_layer = tf.nn.relu(conv_layer)
# Apply Max Pooling
conv_layer = tf.nn.max_pool(
    conv_layer,
    ksize=[1, 2, 2, 1],
    strides=[1, 2, 2, 1],
    padding='SAME')
```

1. ksize：parameter as the size of the filter
2. strides：parameter as the length of the stride

The ksize and strides parameters are structured as 4-element lists, with each element corresponding to a dimension of the input tensor ([batch, height, width, channels]). For both ksize and strides, the batch and channel dimensions are typically set to 1.

A pooling layer is generally used to：
1. Decrease the size of the output
2. Prevent overfitting

Reducing overfitting is a consequence of the reducing the output size, which in turn, reduces the number of parameters in future layers.

但是最近，池化技术逐渐失宠，主要是：

1. Dropout能提供更好的正则化。（Dropout是一种在学习的过程中随机删除神经元的方法，训练时，随机选出隐藏层的神经元，然后将其删除，被删除的神经元不再进行信号的传递。）
2. 池化导致了信息丢失。


# 23. Quiz: Pooling Mechanics

关于池化层和input层的定义如下：

```
- We have an input of shape 4x4x5 (HxWxD)
- Filter of shape 2x2 (HxW)
- A stride of 2 for both the height and width (S)
```

最终处理后的结果公式如下：

```
new_height = (input_height - filter_height)/S + 1
new_width = (input_width - filter_width)/S + 1
```

![image](https://user-images.githubusercontent.com/18595935/52162436-54167600-2717-11e9-91b0-e98cb379a04a.png)

output is `2x2x5`，相应的代码如下：

```python
input = tf.placeholder(tf.float32, (None, 4, 4, 5))
filter_shape = [1, 2, 2, 1]
strides = [1, 2, 2, 1]
padding = 'VALID'
pool = tf.nn.max_pool(input, filter_shape, strides, padding)
```

最后输出的pool应该是`[1, 2, 2, 5]`。

# 29. 1x1 Convolutions

udacity这个视频讲的啥啊，...无语了，完全没懂，看看吴恩达讲的视频[Neural Networks - Networks in Networks and 1x1 Convolutions](https://www.youtube.com/watch?v=vcp0XvDAX68&t=138s)

# 31. Convolutional Network in TensorFlow

## 31.1 示例代码

```python
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(".", one_hot=True, reshape=False)

import tensorflow as tf

# Parameters
learning_rate = 0.00001
epochs = 10
batch_size = 128

# Number of samples to calculate validation and accuracy
# Decrease this if you're running out of memory to calculate accuracy
test_valid_size = 256

# Network Parameters
n_classes = 10  # MNIST total classes (0-9 digits)
dropout = 0.75  # Dropout, probability to keep units


# Store layers weight & bias
weights = {
    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),
    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),
    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),
    'out': tf.Variable(tf.random_normal([1024, n_classes]))}

biases = {
    'bc1': tf.Variable(tf.random_normal([32])),
    'bc2': tf.Variable(tf.random_normal([64])),
    'bd1': tf.Variable(tf.random_normal([1024])),
    'out': tf.Variable(tf.random_normal([n_classes]))}


def conv2d(x, W, b, strides=1):
    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')
    x = tf.nn.bias_add(x, b)
    return tf.nn.relu(x)

def maxpool2d(x, k=2):
    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1], padding='SAME')

def conv_net(x, weights, biases, dropout):
    # Layer 1 - 28*28*1 to 14*14*32
    conv1 = conv2d(x, weights['wc1'], biases['bc1'])
    conv1 = maxpool2d(conv1, k=2)

    # Layer 2 - 14*14*32 to 7*7*64
    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])
    conv2 = maxpool2d(conv2, k=2)

    # Fully connected layer - 7*7*64 to 1024
    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])
    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])
    fc1 = tf.nn.relu(fc1)
    fc1 = tf.nn.dropout(fc1, dropout)

    # Output Layer - class prediction - 1024 to 10
    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])
    return out

# tf Graph input
x = tf.placeholder(tf.float32, [None, 28, 28, 1])
y = tf.placeholder(tf.float32, [None, n_classes])
keep_prob = tf.placeholder(tf.float32)

# Model
logits = conv_net(x, weights, biases, keep_prob)

# Define loss and optimizer
cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))
optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(cost)

# Accuracy
correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

# Initializing the variables
init = tf.global_variables_initializer()

# Launch the graph
with tf.Session() as sess:
    sess.run(init)

    for epoch in range(epochs):
        for batch in range(mnist.train.num_examples//batch_size):
            batch_x, batch_y = mnist.train.next_batch(batch_size)
            sess.run(optimizer, feed_dict={x: batch_x, y: batch_y, keep_prob: dropout})

            # Calculate batch loss and accuracy
            loss = sess.run(cost, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.})
            valid_acc = sess.run(accuracy, feed_dict={
                x: mnist.validation.images[:test_valid_size],
                y: mnist.validation.labels[:test_valid_size],
                keep_prob: 1.})

            print('Epoch {:>2}, Batch {:>3} - Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(
                epoch + 1,
                batch + 1,
                loss,
                valid_acc))

    # Calculate Test Accuracy
    test_acc = sess.run(accuracy, feed_dict={
        x: mnist.test.images[:test_valid_size],
        y: mnist.test.labels[:test_valid_size],
        keep_prob: 1.})
    print('Testing Accuracy: {}'.format(test_acc))
```

## 31.2 代码分析：

**导入数据集并定义各种超参数：**

```python
from tensorflow.examples.tutorials.mnist import input_data
mnist = input_data.read_data_sets(".", one_hot=True, reshape=False)

import tensorflow as tf

# Parameters
learning_rate = 0.00001
epochs = 10
batch_size = 128

# Number of samples to calculate validation and accuracy
# Decrease this if you're running out of memory to calculate accuracy
test_valid_size = 256

# Network Parameters
n_classes = 10  # MNIST total classes (0-9 digits)
dropout = 0.75  # Dropout, probability to keep units
```

**Weights and Biases:**

```python
# Store layers weight & bias
weights = {
    'wc1': tf.Variable(tf.random_normal([5, 5, 1, 32])),
    'wc2': tf.Variable(tf.random_normal([5, 5, 32, 64])),
    'wd1': tf.Variable(tf.random_normal([7*7*64, 1024])),
    'out': tf.Variable(tf.random_normal([1024, n_classes]))}

biases = {
    'bc1': tf.Variable(tf.random_normal([32])),
    'bc2': tf.Variable(tf.random_normal([64])),
    'bd1': tf.Variable(tf.random_normal([1024])),
    'out': tf.Variable(tf.random_normal([n_classes]))}
```

**Convolutions：**

In TensorFlow, this is all done using tf.nn.conv2d() and tf.nn.bias_add().

```python
def conv2d(x, W, b, strides=1):
    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')
    x = tf.nn.bias_add(x, b)
    return tf.nn.relu(x)
```

**Max Pooling：**

![image](https://user-images.githubusercontent.com/18595935/52165001-285ab680-273e-11e9-9264-a3f1ad2976a6.png)

```python
def maxpool2d(x, k=2):
    return tf.nn.max_pool(
        x,
        ksize=[1, k, k, 1],
        strides=[1, k, k, 1],
        padding='SAME')
```

**Model：**

![image](https://user-images.githubusercontent.com/18595935/52165126-dd41a300-273f-11e9-88c7-e7e42ce83011.png)

下面的代码中创建了三层网络，下面代码的注释中解释了每一层中神经元维度的变化，Layer1中，先通过conv2d将28*28*1转换成28*28*32，再应用max pool，将其转换为14*14*32。（32应该表示是通道，将一副图分割了32个通道）

```python
def conv_net(x, weights, biases, dropout):
    # Layer 1 - 28*28*1 to 14*14*32
    conv1 = conv2d(x, weights['wc1'], biases['bc1'])
    conv1 = maxpool2d(conv1, k=2)

    # Layer 2 - 14*14*32 to 7*7*64
    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])
    conv2 = maxpool2d(conv2, k=2)

    # Fully connected layer - 7*7*64 to 1024
    fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])
    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])
    fc1 = tf.nn.relu(fc1)
    fc1 = tf.nn.dropout(fc1, dropout)

    # Output Layer - class prediction - 1024 to 10
    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])
    return out
```

**Session：**

```python
# tf Graph input
x = tf.placeholder(tf.float32, [None, 28, 28, 1])
y = tf.placeholder(tf.float32, [None, n_classes])
keep_prob = tf.placeholder(tf.float32)

# Model
logits = conv_net(x, weights, biases, keep_prob)

# Define loss and optimizer
cost = tf.reduce_mean(\
    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))
optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\
    .minimize(cost)

# Accuracy
correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))
accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))

# Initializing the variables
init = tf. global_variables_initializer()

# Launch the graph
with tf.Session() as sess:
    sess.run(init)

    for epoch in range(epochs):
        for batch in range(mnist.train.num_examples//batch_size):
            batch_x, batch_y = mnist.train.next_batch(batch_size)
            sess.run(optimizer, feed_dict={
                x: batch_x,
                y: batch_y,
                keep_prob: dropout})

            # Calculate batch loss and accuracy
            loss = sess.run(cost, feed_dict={
                x: batch_x,
                y: batch_y,
                keep_prob: 1.})
            valid_acc = sess.run(accuracy, feed_dict={
                x: mnist.validation.images[:test_valid_size],
                y: mnist.validation.labels[:test_valid_size],
                keep_prob: 1.})

            print('Epoch {:>2}, Batch {:>3} -'
                  'Loss: {:>10.4f} Validation Accuracy: {:.6f}'.format(
                epoch + 1,
                batch + 1,
                loss,
                valid_acc))

    # Calculate Test Accuracy
    test_acc = sess.run(accuracy, feed_dict={
        x: mnist.test.images[:test_valid_size],
        y: mnist.test.labels[:test_valid_size],
        keep_prob: 1.})
    print('Testing Accuracy: {}'.format(test_acc))
```

# 32. TensorFlow Convolutional Layer Workspaces

**示例代码：**

```python
import tensorflow as tf
import numpy as np
```

```python
"""
Setup the strides, padding and filter weight/bias such that
the output shape is (1, 2, 2, 3).
"""
# `tf.nn.conv2d` requires the input be 4D (batch_size, height, width, depth)
# (1, 4, 4, 1)
x = np.array([
    [0, 1, 0.5, 10],
    [2, 2.5, 1, -8],
    [4, 0, 5, 6],
    [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))
X = tf.constant(x)

def conv2d(input_array):
    # Filter (weights and bias)
    # The shape of the filter weight is (height, width, input_depth, output_depth)
    # The shape of the filter bias is (output_depth,)
    # TODO: Define the filter weights `F_W` and filter bias `F_b`.
    # NOTE: Remember to wrap them in `tf.Variable`, they are trainable parameters after all.
    F_W = tf.Variable(tf.random_normal([2, 2, 1, 3]))
    F_b = tf.Variable(tf.random_normal([3]))
    # TODO: Set the stride for each dimension (batch_size, height, width, depth)
    strides = [1, 2, 2, 1]
    # TODO: set the padding, either 'VALID' or 'SAME'.
    padding = 'VALID'
    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#conv2d
    # `tf.nn.conv2d` does not include the bias computation so we have to add it ourselves after.
    return tf.nn.conv2d(input_array, F_W, strides, padding) + F_b

output = conv2d(X)
output
```

输出`<tf.Tensor 'add_12:0' shape=(1, 2, 2, 3) dtype=float32>`

```python
##### Do Not Modify ######

import grader

test_X = tf.constant(np.random.randn(1, 4, 4, 1), dtype=tf.float32)

try:
    response = grader.run_grader(test_X, conv2d)
    print(response)
    
    
except Exception as err:
    print(str(err))
    
```

输出：

```
Great job! Your Convolution layer looks good :)
```

上面需要填的是：

```python
def conv2d(input):
    # Filter (weights and bias)
    F_W = tf.Variable(tf.truncated_normal((2, 2, 1, 3)))
    F_b = tf.Variable(tf.zeros(3))
    strides = [1, 2, 2, 1]
    padding = 'VALID'
    return tf.nn.conv2d(input, F_W, strides, padding) + F_b
```

用计算图可以表示如下：

![image](https://user-images.githubusercontent.com/18595935/52170892-f9702f00-2795-11e9-9f1a-7775732be463.png)

1. 因为output的depth是3，所以这里滤波器(权重)应该有3个，同理偏置也是3个
2. 因为output的形状是(2,2)，所以根据公式计算，滤波器的形状也是(2,2)

VALID时，计算公式如下：

```python
out_height = ceil(float(in_height - filter_height + 1) / float(strides[1]))
out_width  = ceil(float(in_width - filter_width + 1) / float(strides[2]))
```

```python
out_height = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2
out_width  = ceil(float(4 - 2 + 1) / float(2)) = ceil(1.5) = 2
```

# 34. TensorFlow Pooling Layer Workspaces

**Using Pooling Layers in TensorFlow:**

In the below exercise, you'll be asked to set up the dimensions of the pooling filters, strides, as well as the appropriate padding. You should go over the TensorFlow documentation for tf.nn.max_pool(). Padding works the same as it does for a convolution.

**Instructions:**
1. Finish off each TODO in the maxpool function.
2. etup the strides, padding and ksize such that the output shape after pooling is (1, 2, 2, 1).

```python
"""
Set the values to `strides` and `ksize` such that
the output shape after pooling is (1, 2, 2, 1).
"""
import tensorflow as tf
import numpy as np

# `tf.nn.max_pool` requires the input be 4D (batch_size, height, width, depth)
# (1, 4, 4, 1)
x = np.array([
    [0, 1, 0.5, 10],
    [2, 2.5, 1, -8],
    [4, 0, 5, 6],
    [15, 1, 2, 3]], dtype=np.float32).reshape((1, 4, 4, 1))
X = tf.constant(x)

def maxpool(input):
    # TODO: Set the ksize (filter size) for each dimension (batch_size, height, width, depth)
    ksize = [1, 2, 2, 1]
    # TODO: Set the stride for each dimension (batch_size, height, width, depth)
    strides = [1, 2, 2, 1]
    # TODO: set the padding, either 'VALID' or 'SAME'.
    padding = 'VALID'
    # https://www.tensorflow.org/versions/r0.11/api_docs/python/nn.html#max_pool
    return tf.nn.max_pool(input, ksize, strides, padding)
    
out = maxpool(X)
```

与上面类似，因为输入数据只有1，所以batch为1，输出数据的depth为1，所以池化层的depth也是1，另外，根据池化计算公式，可以计算出池化层的块大小和步长，都是(2,2)

> filter_height / filter_width 是池化层的块大小，S是步长

```
new_height = (input_height - filter_height)/S + 1
new_width = (input_width - filter_width)/S + 1
```

# 36. Lab: LeNet in TensorFlow

**Preprocessing:**

1. An MNIST image is initially 784 features(1D)
2. If the data is not normalized from [0,255] to [0,1],normalize it.
3. We reshape this to (28,28,1)(3D), and pad the image with 0s such that the height and width are 32.
4. The input shape going into the first convolutional layer is (32，32,1)

**Spec：**
1. Convolution layer 1. The output shape should be 28x28x6.
2. Activation 1. Your choice of activation function.
3. Pooling layer 1. The output shape should be 14x14x6.
4. Convolution layer 2. The output shape should be 10x10x16.
5. Activation 2. Your choice of activation function.
6. Pooling layer 2. The output shape should be 5x5x16.
7. Flatten layer. Flatten the output shape of the final pooling layer such that it's 1D instead of 3D. The easiest way to do is by using tf.contrib.layers.flatten, which is already imported for you.
8. Fully connected layer 1. This should have 120 outputs.
9. Activation 3. Your choice of activation function.
10. Fully connected layer 2. This should have 84 outputs.
11. Activation 4. Your choice of activation function.
12. Fully connected layer 3. This should have 10 outputs.

You'll return the result of the final fully connected layer from the LeNet function.

If implemented correctly you should see output similar to the following:

```
EPOCH 1 ...
Validation loss = 52.809
Validation accuracy = 0.864

EPOCH 2 ...
Validation loss = 24.749
Validation accuracy = 0.915

EPOCH 3 ...
Validation loss = 17.719
Validation accuracy = 0.930

EPOCH 4 ...
Validation loss = 12.188
Validation accuracy = 0.943

EPOCH 5 ...
Validation loss = 8.935
Validation accuracy = 0.954

EPOCH 6 ...
Validation loss = 7.674
Validation accuracy = 0.956

EPOCH 7 ...
Validation loss = 6.822
Validation accuracy = 0.956

EPOCH 8 ...
Validation loss = 5.451
Validation accuracy = 0.961

EPOCH 9 ...
Validation loss = 4.881
Validation accuracy = 0.964

EPOCH 10 ...
Validation loss = 4.623
Validation accuracy = 0.964

Test loss = 4.726
Test accuracy = 0.962

```