---
layout: post
title: Nano01(自動運転)-U03-Lesson13-Deep Neural Networks深度神经网络
date: 2019-01-01 02:03:04
categories: self-driving(自動運転)
tags: self-driving
---
* content
{:toc}

# 3. Number of Parameters

计算下面网络中的参数个数：

![image](https://user-images.githubusercontent.com/18595935/52047828-374e3700-258d-11e9-9ea8-2440bb25f38b.png)

上面的参数，有权重和偏置，权重(input,label)，偏置(label,)

```
= size of W + size of b
= 28x28x10 + 10
= 7850

```

可以参考如下代码：

```python
n_features = 3
n_labels = 5
weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))

bias = tf.Variable(tf.zeros(n_labels))
```

# 7. 2-Layer Neural Network

![image](https://user-images.githubusercontent.com/18595935/52048269-4ed9ef80-258e-11e9-8f91-c30cb72986df.png)
> ReLU 是个非线性函数，当x大于0时，y等于x；否则y为0，该函数的导数如下图：
> ![image](https://user-images.githubusercontent.com/18595935/52048364-906a9a80-258e-11e9-9027-140ea2fa6041.png)

1. 第一层由输入x和其对应的权重w及偏置bias构成，结果经由ReLU函数，传递给下一层神经网络。
2. 第二层由上一层的中间结果，以及该层的权重w和偏置bias构成，计算出来的结果，最终传递给激活函数如softmax函数，计算出概率。

# 8. Quiz: TensorFlow ReLu

ReLU函数(f(x) = max(0, x))，也是一种激活函数，它在TensorFlow中用` tf.nn.relu()`来定义，示例代码如下：

```python
# Hidden Layer with ReLU activation function
hidden_layer = tf.add(tf.matmul(features, hidden_weights), hidden_biases)
hidden_layer = tf.nn.relu(hidden_layer)

output = tf.add(tf.matmul(hidden_layer, output_weights), output_biases)
```

上面的代码：

1. 将 tf.nn.relu() 应用到了 hidden_layer 隐藏层。
2. 添加了一个新的层output layer，output layer的输入数据是前一层hidden_layer的输出(非线性Relu函数处理后的)

In this quiz, you'll use TensorFlow's ReLU function to turn the linear model below into a nonlinear model.

代码如下：

```python
# Solution is available in the other "solution.py" tab
import tensorflow as tf

output = None
hidden_layer_weights = [
    [0.1, 0.2, 0.4],
    [0.4, 0.6, 0.6],
    [0.5, 0.9, 0.1],
    [0.8, 0.2, 0.8]]
out_weights = [
    [0.1, 0.6],
    [0.2, 0.1],
    [0.7, 0.9]]

# Weights and biases
weights = [
    tf.Variable(hidden_layer_weights),
    tf.Variable(out_weights)]
biases = [
    tf.Variable(tf.zeros(3)),
    tf.Variable(tf.zeros(2))]

# Input
features = tf.Variable([[1.0, 2.0, 3.0, 4.0], [-1.0, -2.0, -3.0, -4.0], [11.0, 12.0, 13.0, 14.0]])
```

```python
# TODO: Create Model
hidden_layer = tf.add(tf.matmul(features, weights[0]), biases[0])

hidden_layer = tf.nn.relu(hidden_layer)

output  = tf.add(tf.matmul(hidden_layer, weights[1]), biases[1])
```

```python
# TODO: save and print session results on a variable named "output"

init = tf.global_variables_initializer()

with tf.Session() as sess:
    # Run the tf.constant operation in the session
    sess.run(init)
    result = sess.run(output)
    print(result)
```

输出结果如下：

```
[[  5.11000013   8.44000053]
 [  0.           0.        ]
 [ 24.01000214  38.23999786]]
```


