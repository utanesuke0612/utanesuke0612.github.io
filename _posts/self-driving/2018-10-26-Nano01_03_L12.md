---
layout: post
title: Nano01(自動運転)-U03-Lesson12-TensorFlow
date: 2019-01-01 02:02:04
categories: self-driving(自動運転)
tags: self-driving
---
* content
{:toc}

![image](https://user-images.githubusercontent.com/18595935/51788078-3d26cf80-21bd-11e9-9fd2-023aaab32ad2.png)

# 6. Installing TensorFlow

Conda中安装TensorFlow：

```python
conda create --name=IntroToTensorFlow python=3 anaconda
source activate IntroToTensorFlow
conda install -c conda-forge tensorflow
```

测试代码1：

```python
import tensorflow as tf

# Create TensorFlow object called tensor
hello_constant = tf.constant('Hello World!')

with tf.Session() as sess:
    # Run the tf.constant operation in the session
    output = sess.run(hello_constant)
    print(output)
```

输出`b'Hello World!'`

测试代码2：

```python
import tensorflow as tf
import numpy as np

# 使用 NumPy 生成假数据(phony data), 总共 100 个点.
x_data = np.float32(np.random.rand(2, 100)) # 随机输入
y_data = np.dot([0.100, 0.200], x_data) + 0.300

# 构造一个线性模型
# 
b = tf.Variable(tf.zeros([1]))
W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0))
y = tf.matmul(W, x_data) + b

# 最小化方差
loss = tf.reduce_mean(tf.square(y - y_data))
optimizer = tf.train.GradientDescentOptimizer(0.5)
train = optimizer.minimize(loss)

# 初始化变量
init = tf.initialize_all_variables()

# 启动图 (graph)
sess = tf.Session()
sess.run(init)

# 拟合平面
for step in range(0, 201):
    sess.run(train)
    if step % 20 == 0:
        print(step, sess.run(W), sess.run(b))

# 得到最佳拟合结果 W: [[0.100  0.200]], b: [0.300]
```

```python
0 [[-0.29607844  0.49112239]] [ 0.74048108]
20 [[-0.0404522   0.21079057]] [ 0.36591059]
40 [[ 0.06075135  0.19667548]] [ 0.32181653]
60 [[ 0.08866727  0.19777994]] [ 0.3069748]
80 [[ 0.09665523  0.19910237]] [ 0.30218849]
100 [[ 0.09899885  0.19968568]] [ 0.3006795]
120 [[ 0.0996977  0.1998966]] [ 0.30020973]
140 [[ 0.09990822  0.19996706]] [ 0.3000645]
160 [[ 0.09997206  0.19998969]] [ 0.3000198]
180 [[ 0.09999147  0.1999968 ]] [ 0.30000606]
200 [[ 0.09999739  0.19999902]] [ 0.30000183]
```

# 7. 分析示例代码：

```python
import tensorflow as tf

# Create TensorFlow object called hello_constant
hello_constant = tf.constant('Hello World!')

with tf.Session() as sess:
    # Run the tf.constant operation in the session
    output = sess.run(hello_constant)
    print(output)
```

**Tensor 张量：**

在TensorFlow中，数据不是以int string等存储，而是被包装在一个叫做tensor的对象中，比如上面的`hello_constant = tf.constant('Hello World!')`，`hello_constant`就是一个0维的string tensor，另外还可以以其他的类型，其他的维度来定义tensor:

```python
# A is a 0-dimensional int32 tensor
A = tf.constant(1234) 
# B is a 1-dimensional int32 tensor
B = tf.constant([123,456,789]) 
# C is a 2-dimensional int32 tensor
C = tf.constant([ [123,456,789], [222,333,444] ])
```

**Session 会话：**

我们把上面运行的代码，可以用下面的图graph来描述：

![image](https://user-images.githubusercontent.com/18595935/51788570-b8d74b00-21c2-11e9-9f66-ea8390b78c8f.png)

一个TensorFlow Session是一个运行上述图的一个环境，session负责将操作指派给GPU或是CPU，比如下面的代码：

```python
with tf.Session() as sess:
    output = sess.run(hello_constant)
    print(output)
```

上面的代码将张量hello_constant中的值计算出来，首先使用`tf.session`生成一个session，再使用`sess.run()`提取出张量`hello_constant`中的值。

```python
# A is a 0-dimensional int32 tensor
A = tf.constant(1234) 
# B is a 1-dimensional int32 tensor
B = tf.constant([123,456,789]) 
# D is a 0-dimensional string tensor
D = tf.constant("Hello World!")

with tf.Session() as sess:
    # Run the tf.constant operation in the session
    output = sess.run(A)
    print("constant(1234):            ",type(output))
    
    output = sess.run(B)
    print("constant([123,456,789]):   ",type(output))
    
    output = sess.run(D)
    print("constant('Hello World!'):  ",type(output))
    
```

```python
constant(1234):             <class 'numpy.int32'>
constant([123,456,789]):    <class 'numpy.ndarray'>
constant('Hello World!'):   <class 'bytes'>
```

# 8. Quiz: Tensorflow Input

上一个章节中，我们将一个tensor传给session然后返回了结果，这里tensor设定的是常量，如果需要使用非常量呢，这里需要使用`tf.placeholder()`和`feed_dict`.

**示例代码：**

```python
x = tf.placeholder(tf.string)

with tf.Session() as sess:
    output = sess.run(x, feed_dict={x: 'Hello World'})
```

```python
x = tf.placeholder(tf.string)

with tf.Session() as sess:
    output = sess.run(x, feed_dict={x: 'Hello World'})
```

# 9. Quiz: Tensorflow Math

**示例代码：**

```python
x = tf.add(5, 2)  # 7
z = tf.subtract(10, 4) # 6
y = tf.multiply(2, 5)  # 10
```

如果是类型不同的进行了运算，会出错：

```python
x = tf.subtract(tf.constant(2.0),tf.constant(1)) 
```

出错为：`TypeError: Input 'y' of 'Sub' Op has type int32 that does not match type float32 of argument 'x'.`

需要修改为：

```python
x = tf.subtract(tf.cast(tf.constant(2.0), tf.int32), tf.constant(1))   # 1
```

**quiz：**

```python
# Quiz Solution
# Note: You can't run code in this tab
import tensorflow as tf

# TODO: Convert the following to TensorFlow:
x = tf.constant(10)
y = tf.constant(2)
z = tf.subtract(tf.divide(x,y),tf.cast(tf.constant(1), tf.float64))

# TODO: Print z from a session
with tf.Session() as sess:
    output = sess.run(z)
    print(output)
```

# 10. Transition to Classification

回顾一下，通过上面的学习，我们掌握了：

- 使用**tf.session**
- 使用**tf.constant()**
- 使用**tf.placeholder()**和**feed_dict**取得input
- 使用数学运算符号**tf.add(), tf.subtract(), tf.multiply(), and tf.divide()**

# 13. Training Your Logistic Classifier 训练逻辑分类器

![image](https://user-images.githubusercontent.com/18595935/51810331-86287200-22ea-11e9-8875-c8e22b346668.png)

- X是输入，作为输入的每一个图像，有且只有一个标签。
- W和b分别是权重和偏置，根据带有标签的图像集，训练出最合适的W和b。
- y是预测结果，y经过激活函数softmax函数转换后，得到其对应的概率。

# 14. TensorFlow Linear Function

假设我们要把图片识别为数字，**y = Wx + b**，x表示pixel的value，y表示识别后得到的数字，W表示权重，是决定x的影响因子。

**Weights and Bias in TensorFlow：**

神经网络训练的最终目的是，得到一个最好的权重weight和偏置bias，其预测结果最大程度与label标签相同。

在TensorFlow中，要使用weights和bias，那定义为的tensor需要是可以修改的，`tf.placeholder()`和`tf.constant()`都不能修改，需要使用`tf.Variabble()`。

- **tf.Variable():**

上述函数生成一个能修改的带有初始值的tensor，这个tensor的状态存储再session中，所以需要手动初始化tensor状态。使用`tf.global_variables_initializer()`函数去初始化所有tensor变量的状态。

示例代码：

```python
n_features = 120
n_labels = 5
weights = tf.Variable(5)
print(weights)

init = tf.global_variables_initializer()

with tf.Session() as sess:
    sess.run(init)
    output = sess.run(weights)
    print(output)
```

输出：

```python
<tf.Variable 'Variable_32:0' shape=() dtype=int32_ref>
5
```

- **weight: tf.truncated_normal()**

上述函数能生成一个正态分布的随机数，示例代码如下：

```python
n_features = 3
n_labels = 5
weights = tf.Variable(tf.truncated_normal((n_features, n_labels)))

init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    output = sess.run(weights)
    print(output)    
    print(output.shape)
```

输出如下：

```python
[[ 1.66512358  0.47815749  0.11390464  0.04166538 -0.59684688]
 [ 1.19306767  1.85732388  1.12105191 -0.04347496  0.31718659]
 [-0.8618989  -0.52991307  0.99672502  0.87029117  0.34897801]]
(3, 5)
```
> features(3) 表示特征数，labels(5)表示输出的标签数目，weight的形状为(3,5)。


- **bias: tf.zeros()**

生成全是0的的tensor：

```python
n_features = 3
n_labels = 5
bias = tf.Variable(tf.zeros(n_labels))

init = tf.global_variables_initializer()
with tf.Session() as sess:
    sess.run(init)
    output = sess.run(bias)
    print(output)    
    print(output.shape)
```

# 15. Quiz: Linear Function

![image](https://user-images.githubusercontent.com/18595935/51813199-70ba4480-22f8-11e9-9a11-d58938f4dc0b.png)

使用TensorFlow识别上述的手写数字图像。相关代码如下：

- 分别随机生成权重和偏置，然后计算预测结果：

```python
# Quiz Solution
import tensorflow as tf

def get_weights(n_features, n_labels):
    """
    Return TensorFlow weights
    :param n_features: Number of features
    :param n_labels: Number of labels
    :return: TensorFlow weights
    """
    # TODO: Return weights
    return tf.Variable(tf.truncated_normal((n_features, n_labels)))


def get_biases(n_labels):
    """
    Return TensorFlow bias
    :param n_labels: Number of labels
    :return: TensorFlow bias
    """
    # TODO: Return biases
    return tf.Variable(tf.zeros(n_labels))


def linear(input, w, b):
    """
    Return linear function in TensorFlow
    :param input: TensorFlow input
    :param w: TensorFlow weights
    :param b: TensorFlow biases
    :return: TensorFlow linear function
    """
    # TODO: Linear Function (xW + b)
    return tf.add(tf.matmul(input, w), b)
```
> Since xW in xW + b is matrix multiplication, you have to use the **tf.matmul()** function instead of **tf.multiply()**. Don't forget that order matters in matrix multiplication, so tf.matmul(a,b) is not the same as tf.matmul(b,a).

- 获取input并计算：

```python
from tensorflow.examples.tutorials.mnist import input_data

def mnist_features_labels(n_labels):
    """
    Gets the first <n> labels from the MNIST dataset
    :param n_labels: Number of labels to use
    :return: Tuple of feature list and label list
    """
    mnist_features = []
    mnist_labels = []

    mnist = input_data.read_data_sets('/datasets/ud730/mnist', one_hot=True)

    # In order to make quizzes run faster, we're only looking at 10000 images
    for mnist_feature, mnist_label in zip(*mnist.train.next_batch(10000)):

        # Add features and labels if it's for the first <n>th labels
        if mnist_label[:n_labels].any():
            mnist_features.append(mnist_feature)
            mnist_labels.append(mnist_label[:n_labels])

    return mnist_features, mnist_labels


# Number of features (28*28 image is 784 features)
n_features = 784
# Number of labels
n_labels = 3

# Features and Labels
features = tf.placeholder(tf.float32)
labels = tf.placeholder(tf.float32)

# Weights and Biases
w = get_weights(n_features, n_labels)
b = get_biases(n_labels)

# Linear Function xW + b
logits = linear(features, w, b)

# Training data
train_features, train_labels = mnist_features_labels(n_labels)
```

注意首先要使用函数 `global_variables_initializer` 进行初始化操作：

```python
with tf.Session() as session:
    session.run(tf.global_variables_initializer())

    # Softmax
    prediction = tf.nn.softmax(logits)

    # Cross entropy
    # This quantifies how far off the predictions were.
    # You'll learn more about this in future lessons.
    cross_entropy = -tf.reduce_sum(labels * tf.log(prediction), reduction_indices=1)

    # Training loss
    # You'll learn more about this in future lessons.
    loss = tf.reduce_mean(cross_entropy)

    # Rate at which the weights are changed
    # You'll learn more about this in future lessons.
    learning_rate = 0.08

    # Gradient Descent
    # This is the method used to train the model
    # You'll learn more about this in future lessons.
    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss)

    # Run optimizer and get loss
    _, l = session.run(
        [optimizer, loss],
        feed_dict={features: train_features, labels: train_labels})

# Print loss
print('Loss: {}'.format(l))
```

# 16. Linear Update 线性更新

![image](https://user-images.githubusercontent.com/18595935/51813859-c47a5d00-22fb-11e9-865a-6cd6bc16d08f.png)

最后需要将bias偏置加到WX上去，这里用到numpy和TensorFlow的广播特性：

```python
import numpy as np
t = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])
u = np.array([1, 2, 3])
print(t + u)
```

```python
[[ 2  4  6]
 [ 5  7  9]
 [ 8 10 12]
 [11 13 15]]
```

针对所有的元素都加了`[1, 2, 3]`。

# 17. Quiz: Softmax

![image](https://user-images.githubusercontent.com/18595935/51814128-28e9ec00-22fd-11e9-84f4-7d04a870318d.png)

上面计算了线性函数的结果，现在需要将结果转换为概率，这里使用softmax(x)函数：

```python
# Solution is available in the other "solution.py" tab
import numpy as np
def softmax(x):
    """Compute softmax values for each sets of scores in x."""
    # TODO: Compute and return softmax(x)
    
    exp_x = np.exp(x)
    sum_exp_x = np.sum(exp_x, axis=0)
    y = exp_x / sum_exp_x
    
    return y

logits = [3.0, 1.0, 0.2]
print(softmax(logits))
```

```python
[0.8360188  0.11314284 0.05083836]
```

# 18. Quiz: TensorFlow Softmax Workspaces

在tensorflow中，有对应的函数直接计算softmax值，代码如下：

```python
x = tf.nn.softmax([3.0, 1.0, 0.2])

with tf.Session() as sess:
    output = sess.run(x)
    print(output)
```

```python
[0.8360188  0.11314284 0.05083836]
```

Quiz的代码：

```python
# Quiz Solution
import tensorflow as tf
def run():
    output = None
    logit_data = [3.0, 1.0, 0.1]
    logits = tf.placeholder(tf.float32)

    softmax = tf.nn.softmax(logits)

    with tf.Session() as sess:
        output = sess.run(softmax, feed_dict={logits: logit_data})

    return output

print(run())
```

输出`[ 0.84008306  0.11369288  0.04622407]`

1. 如果将计算的结果，乘以10，则随后softmax计算出来的概率，要么趋于0要么趋于1。

2. 如果将计算结果除以10，那最终的概率呈现均已分布。

下面是分别乘以10和除以10的结果：

```python
[  1.00000000e+00   2.06115369e-09   2.54366569e-13]

[ 0.32757813  0.35133022  0.32109165]
```

# 19. One-Hot Encoding

![image](https://user-images.githubusercontent.com/18595935/51814549-22f50a80-22ff-11e9-84dd-9520969880dd.png)

# 21. Cross Entropy 交叉熵

> [交叉熵误差](http://road2ai.info/2018/07/28/Deeplearning_04/#22-%E4%BA%A4%E5%8F%89%E7%86%B5%E8%AF%AF%E5%B7%AEcross-entropy-error-%E8%AF%84%E4%BB%B7)
> 

衡量两个向量之间的距离的方法，称为交叉熵。

![image](https://user-images.githubusercontent.com/18595935/51816549-5b4e1600-230a-11e9-9d36-ae0aafa3ae58.png)

# 22. Minimizing Cross Entropy

![image](https://user-images.githubusercontent.com/18595935/51816741-6786a300-230b-11e9-8088-1b2182d8e6d4.png)

这个损失函数，求出了对于所有训练集样本的交叉熵的均值，损失函数是一个关于权值和偏置的函数，我们希望该损失函数的值最小。(说明预测值越靠近标签值)

# 23. Practical Aspects of Learning

下面介绍一些计算导数的工具，以及梯度下降法的优缺点。在训练第一个模型时，有如下两个问题需要解决：
1. 如何把图像像素输入到分类器(How do you fill image pixels to this classfier?)
2. 在哪里初始化最优化过程(Where do you initialize the optimization?)

# 24. Quiz: Numerical Stability

数值计算时，需要考虑极大值和极小值的问题，如下的示例代码：

```python
a = 1000000000
for i in range(1000000):
    a = a + 1e-6
print(a - 1000000000)
```

得到的结果是`0.953674316406`，与预期值`1`不同，说明计算时产生了误差。

# 25. Normalized Inputs and Initial Weights

**正规化input数据：**

好的输入数据能够减少优化过程，好的输入数据有两个特点：
1. 平均值接近0
2. 较小的方差

![image](https://user-images.githubusercontent.com/18595935/51817483-43789100-230e-11e9-8d3b-517133751d55.png)

基于上面的原则，将图像数据的RGB值进行如下的处理：

![image](https://user-images.githubusercontent.com/18595935/51817489-4a9f9f00-230e-11e9-8bc4-d394a65de3b2.png)

**权值初始值生成：**

理想的权值和偏置初始值，有利于做梯度下降。一种简单的方法是：
从高斯分布上随机获取初始权重，使这些权重的均值为0，标准差为σ（是离均差平方的算术平均数的平方根）。

完成上面的工作后，就可以进行训练了。
> x表示输入，W表示权重，b表示偏置，S()表示softmax函数，Li表示标签值，D()表示交叉熵，最后对交叉熵取平均值，得到最后结果。

![image](https://user-images.githubusercontent.com/18595935/51817663-f1843b00-230e-11e9-9fc0-94269121e684.png)

# 26. Measuring Performance

![image](https://user-images.githubusercontent.com/18595935/51819235-05cb3680-2315-11e9-97ff-5a0e5b285563.png)

将数据集分为训练集，验证集和测试集。

# 27. Transition: Overfitting 

了解下什么是交叉验证(Cross-Validation)，交叉验证是深度学习的基础之一。

交叉验证是一种评估 ML 模型的方法，具体方法是通过使用可用输入数据子集训练多个 ML 模型并使用补充数据子集对其进行评估。使用交叉验证来检测过度拟合，即无法泛化模式。

![image](https://user-images.githubusercontent.com/18595935/51819778-ef25df00-2316-11e9-8b27-cc30d0e7f7e0.png)

上面的练习中，小于30个的改变，都有可能是噪声导致的，超过30个才可能是调整权重的结果，所以只有第一个选项是yes。


# 32. Stochastic Gradient Descent 随机梯度下降

随机梯度下降(S.G.D)是深度学习的核心知识点，随机梯度下降能够适应各种数据和模型的大小。

# 33. Momentum and Learning Rate Decay 

> Momentum and Learning Rate Decay 动量和学习速率衰减


**这后面都是关于学习相关的技巧，视频讲解太概略了，先看深度那本书后，再回过头来学...**

